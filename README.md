# Deep-Learning-Echelon
A metaphoric "echelon" (learning process) towards the bottom of deep learning.

```
[2024-05-22/(42)-days]

1. Model:
   1. RNN
   2. LSTM
   3. GRU
   4. bi-RNN
   5. CNN(+Fully)
   6. ResNet
   7. AlexNet
   8. DenseNet
   9. LeNet
   10. MobileNet
   11. Network-in-network
   12. VGG
   13. AE, VAE, Con-VAE
   14. GAN
   15. GNN
   16. self-attention
   17. Transformers

2. Trick:
   1. DropOut
   2. BN
   3. K-Fold Cross-Validation
   4. Data Augmentation
   5. Cyclical Learning Rate
   6. Annealing with Increasing the Batch Size
   7. Gradient Clipping

3. Pytorch-workflow:
   1. TensorBoard -- continue training the last model
   2. TensorBoard -- checkpointing best model
   3. Preprocessing: 
      DataLoader State and Nested Iterations
      Generating Validation Set Splits
      Dataloading with Pinned Memory
      Standardization
   4. Gradient Checkpointing Demo   
   5. Weight Sharing Within a Layer
   6. Getting Gradients of an Intermediate Variable in PyTorch
   
4. Visualisation
   1. Vanilla Loss Gradient (wrt Inputs) Visualization
   2. Guided Backpropagation
   3. Plotting Live Training Performance in Jupyter Notebooks with just Matplotlib
```

